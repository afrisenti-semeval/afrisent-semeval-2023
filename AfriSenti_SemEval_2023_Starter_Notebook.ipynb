{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V4UObc2ql-Zd",
        "jW99ncOhhDEm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/afrisenti-logo.png\" width=\"30%\" />\n",
        "</center>"
      ],
      "metadata": {
        "id": "AEkdLm_JF84s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "#SemEval 2023 Shared Task 12: AfriSenti\n",
        "\n",
        "###Starter Notebook\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "pXY0zNpmLM1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leveraging Pre-trained A Language Model to Train A Sentiment Classifier\n",
        "\n",
        "**Authors:**\n",
        "[Idris Abdulmumin](https://www.hausanlp.org/author/idris-abdulmuminu/), [David Adelani](https://dadelani.github.io/) and [Shamsuddeen Hassan Muhammad](https://www.hausanlp.org/author/shamsuddeen-hassan-muhammad/).\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "You are welcome to participate in our first-of-its-kind SemEval Shared Task! \n",
        "\n",
        "In this starter notebook, we will take you through the process of fine-tuning a pre-trained language model on a sample data to build a sentiment classifier. The notebook was adapted from a [Hugginface implementation]( https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_xnli.py) for such tasks.\n",
        "\n",
        "**Level:** <font color='blue'>`Beginner to Intermediate`</font>\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "1. Installation and importation of necessary libraries\n",
        "2. Setting up the project parameters.\n",
        "3. Running training and evaluation\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "It is **strongly advised** that you use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n",
        "NB: \n",
        "\n",
        "- **The codes in this notebook are provided to familiarize yourselves with fine-tuning language models for sentiment classification. You may extend and (or) modify as appropriate to obtain competitive performances**\n",
        "\n",
        "- **We also use the data as it is, without any cleaning such as removal of emoji and hyperlinks.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jURfhK3zGM9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Installations and imports"
      ],
      "metadata": {
        "id": "V4BB4JDx5W8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##a. Mount drive (if you are running on colab)"
      ],
      "metadata": {
        "id": "DObkW3ulM7yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z0IelbK7NBZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8d3182-ccc9-46a4-9b07-4374df70d924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b. Clone competition repository\n",
        "\n",
        "After cloning, under MyDrive, you will see afrisenti-semeval-2023 folder with all the the data for the afrisenti shared task (training and dev) "
      ],
      "metadata": {
        "id": "U-OZxUWIMqtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "\n",
        "!git clone https://github.com/afrisenti-semeval/afrisent-semeval-2023.git"
      ],
      "metadata": {
        "id": "2392TKadMvqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a3c426-da7a-4bbd-d754-acf52af98170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Cloning into 'afrisent-semeval-2023'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 133 (delta 53), reused 73 (delta 21), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (133/133), 6.68 MiB | 14.76 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##c. Install required libraries\n",
        "\n",
        "- Set the project dire\n",
        "ctory in the cell below, where the requirements file should also be located, and run the cell"
      ],
      "metadata": {
        "id": "Xb03Gp9fUN8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_DIR = '/content/drive/MyDrive/afrisent-semeval-2023'\n",
        "import os\n",
        "if os.path.isdir(PROJECT_DIR):\n",
        "  %cd {PROJECT_DIR}\n",
        "else:\n",
        "  print(\"Project directory not found, please check again.\")\n",
        "\n",
        "#The requirements file should be in PROJECT_DIR\n",
        "if os.path.isfile('starter_kit/requirements.txt'):\n",
        "  !pip install -r starter_kit/requirements.txt\n",
        "else:\n",
        "  print('requirements.txt file not found')"
      ],
      "metadata": {
        "id": "4Cbmi_mQ4k3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e901bedc-cdb2-498f-940a-99f33f271bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/afrisent-semeval-2023\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r starter_kit/requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r starter_kit/requirements.txt (line 2)) (1.21.6)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.9 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r starter_kit/requirements.txt (line 4)) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r starter_kit/requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r starter_kit/requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r starter_kit/requirements.txt (line 7)) (1.0.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.12.0-py3-none-any.whl (143 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 57.7 MB/s \n",
            "\u001b[?25hCollecting datasets>=1.8.0\n",
            "  Downloading datasets-2.5.1-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 431 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.3.5.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2022.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (21.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115 kB 55.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (4.12.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r starter_kit/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate->-r starter_kit/requirements.txt (line 8)) (5.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.8.1)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, tokenizers, datasets, transformers, sentencepiece, evaluate, accelerate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed accelerate-0.12.0 datasets-2.5.1 evaluate-0.2.2 huggingface-hub-0.10.0 multiprocess-0.70.13 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.2 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##d. Import libraries\n",
        "\n",
        "Import libraries below"
      ],
      "metadata": {
        "id": "2zszKhh2Ufb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8QIl420aUM1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Dataset"
      ],
      "metadata": {
        "id": "SoRyMJMDJ7lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##a. Formatting\n",
        "\n",
        "The training dataset that was provided for the competition is in the following format:\n",
        "\n",
        "| ID | text | label |\n",
        "| --- | --- | --- |\n",
        "| twt001 | example text | negative |\n",
        "| twt002 | example text | positive |\n",
        "| ... | ... | ... |\n",
        "\n",
        "However, the code in the starter kit do not expect the \n",
        "ID and require the training (and evaluation) data to be in the following format\n",
        "\n",
        "|text | label |\n",
        "|--- | --- |\n",
        "|example text | negative |\n",
        "|example text | positive |\n",
        "|... | ... |\n",
        "\n",
        "To reformat the data run the following cell"
      ],
      "metadata": {
        "id": "3wk5vMnrXMSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DATA_DIR = '/content/drive/MyDrive/afrisent-semeval-2023/datasets/train'\n",
        "\n",
        "if os.path.isdir(TRAINING_DATA_DIR):\n",
        "  FORMATTED_TRAIN_DATA = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data')\n",
        "  print('Data directory found.')\n",
        "  if not os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "    print('Creating directory to store formatted data.')\n",
        "    os.mkdir(FORMATTED_TRAIN_DATA)\n",
        "else:\n",
        "  print(TRAINING_DATA_DIR + ' is not a valid directory or does not exist!')\n",
        "\n",
        "%cd {TRAINING_DATA_DIR}\n",
        "\n",
        "training_files = os.listdir()\n",
        "\n",
        "if len(training_files) > 0:\n",
        "  for training_file in training_files:\n",
        "    if training_file.endswith('.tsv'):\n",
        "\n",
        "      data = training_file.split('_')[0]\n",
        "      if not os.path.isdir(os.path.join(FORMATTED_TRAIN_DATA, data)):\n",
        "        print(data, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(FORMATTED_TRAIN_DATA, data))\n",
        "      \n",
        "      df = pd.read_csv(training_file, sep='\\t', names=['ID', 'text', 'label'], header=0)\n",
        "      df[['text', 'label']].to_csv(os.path.join(FORMATTED_TRAIN_DATA, data, 'train.tsv'), sep='\\t', index=False)\n",
        "    \n",
        "    else:\n",
        "      print(training_file + ' is not a supported file!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ],
      "metadata": {
        "id": "ssyIZOUJMrzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0940596-30fa-4fed-ab17-f69832fa98d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory found.\n",
            "Creating directory to store formatted data.\n",
            "/content/drive/MyDrive/afrisent-semeval-2023/datasets/train\n",
            "am Creating directory to store train, dev and test splits.\n",
            "dz Creating directory to store train, dev and test splits.\n",
            "ha Creating directory to store train, dev and test splits.\n",
            "ig Creating directory to store train, dev and test splits.\n",
            "ma Creating directory to store train, dev and test splits.\n",
            "multilingual Creating directory to store train, dev and test splits.\n",
            "pcm Creating directory to store train, dev and test splits.\n",
            "pt Creating directory to store train, dev and test splits.\n",
            "sw Creating directory to store train, dev and test splits.\n",
            "yo Creating directory to store train, dev and test splits.\n",
            "formatted-train-data is not a supported file!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the code above, a new folder (called formated-train-data) with formated files is created in the \"datasets\" folder in the train sub-folder."
      ],
      "metadata": {
        "id": "7S2Dup8GHl1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b. <font color='red'>`(Optional) Creating Evaluation (Dev and Test) sets from the available training data`</font>\n",
        "\n",
        "You may wish to create train and evaluation (dev and test) sets from the training data provided. If you wish to do so, you can run any of the cells below`"
      ],
      "metadata": {
        "id": "5LgeVN_wXGrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###i. If you want to create both the Dev and Test sets, run this cell"
      ],
      "metadata": {
        "id": "APxVxL06lfux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "  print('Data directory found.')\n",
        "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test')\n",
        "  if not os.path.isdir(SPLITTED_DATA):\n",
        "    print('Creating directory to store train, dev and test splits.')\n",
        "    os.mkdir(SPLITTED_DATA)\n",
        "else:\n",
        "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
        "\n",
        "%cd {FORMATTED_TRAIN_DATA}\n",
        "formatted_training_files = os.listdir()\n",
        "\n",
        "if len(formatted_training_files) > 0:\n",
        "  for data_name in formatted_training_files:\n",
        "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
        "    if os.path.isfile(formatted_training_file):\n",
        "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
        "      train, dev, test = np.split(labeled_tweets.sample(frac=1, random_state=42), [int(.7*len(labeled_tweets)), int(.8*len(labeled_tweets))])\n",
        "\n",
        "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
        "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
        "\n",
        "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
        "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
        "      test.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name,'test.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' is not a supported file!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ],
      "metadata": {
        "id": "aVq1Blz0YF2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee6ba80-e926-46d8-a4ca-5da5e20e2300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory found.\n",
            "Creating directory to store train, dev and test splits.\n",
            "/content/drive/MyDrive/afrisent-semeval-2023/datasets/train/formatted-train-data\n",
            "am Creating directory to store train, dev and test splits.\n",
            "dz Creating directory to store train, dev and test splits.\n",
            "ha Creating directory to store train, dev and test splits.\n",
            "ig Creating directory to store train, dev and test splits.\n",
            "ma Creating directory to store train, dev and test splits.\n",
            "multilingual Creating directory to store train, dev and test splits.\n",
            "pcm Creating directory to store train, dev and test splits.\n",
            "pt Creating directory to store train, dev and test splits.\n",
            "sw Creating directory to store train, dev and test splits.\n",
            "yo Creating directory to store train, dev and test splits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the code above, a new folder (called splitted-train-dev-test) with train-dev-test split is created in the \"datasets\" folder in the train sub-folder. Here, the train-dev-test split is 70/10/20\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cZy_rzouJZFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ii. If you want to create only the Dev set from the training data, please run this"
      ],
      "metadata": {
        "id": "V4UObc2ql-Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "  print('Data directory found.')\n",
        "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev')\n",
        "  if not os.path.isdir(SPLITTED_DATA):\n",
        "    print('Creating directory to store train, dev and test splits.')\n",
        "    os.mkdir(SPLITTED_DATA)\n",
        "else:\n",
        "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
        "\n",
        "%cd {FORMATTED_TRAIN_DATA}\n",
        "formatted_training_files = os.listdir()\n",
        "\n",
        "if len(formatted_training_files) > 0:\n",
        "  for data_name in formatted_training_files:\n",
        "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
        "    if os.path.isfile(formatted_training_file):\n",
        "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
        "      train, dev = train_test_split(labeled_tweets, test_size=0.3)\n",
        "\n",
        "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
        "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
        "\n",
        "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
        "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' is not a supported file!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ],
      "metadata": {
        "id": "tU24jW_gmFu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd8510b-a4f9-4164-ce9c-177c2d8a7daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory found.\n",
            "Creating directory to store train, dev and test splits.\n",
            "/content/drive/MyDrive/afrisent-semeval-2023/datasets/train/formatted-train-data\n",
            "am Creating directory to store train, dev and test splits.\n",
            "dz Creating directory to store train, dev and test splits.\n",
            "ha Creating directory to store train, dev and test splits.\n",
            "ig Creating directory to store train, dev and test splits.\n",
            "ma Creating directory to store train, dev and test splits.\n",
            "multilingual Creating directory to store train, dev and test splits.\n",
            "pcm Creating directory to store train, dev and test splits.\n",
            "pt Creating directory to store train, dev and test splits.\n",
            "sw Creating directory to store train, dev and test splits.\n",
            "yo Creating directory to store train, dev and test splits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the code above, a new folder (called splitted-train-dev) with train-dev split is created in the \"datasets\" folder in the train sub-folder. Here, the train-dev split is 70/30\n"
      ],
      "metadata": {
        "id": "usfr00QhKSnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Training setup"
      ],
      "metadata": {
        "id": "aDoyRlje3Rm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##a. Set project parameters\n",
        "\n",
        "For a list of models that be used for fine-tuning, you can check [HERE](https://huggingface.co/models)."
      ],
      "metadata": {
        "id": "8AaXec415s0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {PROJECT_DIR}\n",
        "\n",
        "# Language to train sentiment classifier for\n",
        "LANGUAGE_CODE = 'ha'\n",
        "\n",
        "# Model Training Parameters\n",
        "MODEL_NAME_OR_PATH = 'Davlan/afro-xlmr-mini'\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-5\n",
        "NUMBER_OF_TRAINING_EPOCHS = 5\n",
        "MAXIMUM_SEQUENCE_LENGTH = 128\n",
        "SAVE_STEPS = -1"
      ],
      "metadata": {
        "id": "M0TKIFrE5ybV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bee9e4e-a710-48ed-88bf-ed46455151f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/afrisent-semeval-2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b. Train the model\n",
        "\n",
        "In the section below, we provide three options: \n",
        "\n",
        "- 1) training model without any validation; \n",
        "- 2) training model with validation but without testing; \n",
        "- 3) training a model with validation and test set."
      ],
      "metadata": {
        "id": "I2qcCQnU8dgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###i. Training on only Train set, without any evaluation"
      ],
      "metadata": {
        "id": "8fEw-qcEhYnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join('/content/drive/MyDrive/afrisent-semeval-2023/models', LANGUAGE_CODE + '_no_eval')\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --learning_rate {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS}"
      ],
      "metadata": {
        "id": "kAgg2MmAhiiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may observe, the training loss is very large. As a start, you can tune the training parameters and model to get a competitive result. \n",
        "\n",
        "You can observe also, there is no validation metrics (e.g., accuracy, loss etc) since we are only training without validtaion "
      ],
      "metadata": {
        "id": "wZtt5lBBLPz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ii. Training on only Train and Dev sets"
      ],
      "metadata": {
        "id": "MpFSMaAzhS5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join('/content/drive/MyDrive/afrisent-semeval-2023/models', LANGUAGE_CODE + '_no_test')\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --learning_rate {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS}"
      ],
      "metadata": {
        "id": "3TMAD_iLhpgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can observe, there is evalidation metrics (e.g., accuracy, loss etc) since we are evaluating our model performance on the validation set we created from the \n",
        "training data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3I2VJlCMJTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###iii. Training with Train, Dev and Test sets"
      ],
      "metadata": {
        "id": "jW99ncOhhDEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaVssq7a1uwk"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join('/content/drive/MyDrive/afrisent-semeval-2023/models', LANGUAGE_CODE)\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --learning_rate {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you trained your best model and find the best  parameters, you can submit your prediction on dev or test set on CodaLab competition page."
      ],
      "metadata": {
        "id": "2lLKHsrEPucP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Submission\n",
        "\n",
        "- For submission after training your model, unlabeled tweets were provided for dev (development phase) and test (evaluation phase). \n",
        "\n",
        "- To generate their sentiment prediction, provide the path to the file containing the unlabeled tweets.\n",
        "\n",
        "**What the code does**\n",
        "1. Predicting sentiments of the unlabeled tweets (dev or test)\n",
        "2. Create a file in the submission format"
      ],
      "metadata": {
        "id": "g9DeJD4CVmEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {PROJECT_DIR}\n",
        "\n",
        "OUTPUT_DIR = os.path.join('/content/drive/MyDrive/afrisent-semeval-2023/models', LANGUAGE_CODE)\n",
        "FILE_NAME = '/content/drive/MyDrive/afrisent-semeval-2023/datasets/dev/'+LANGUAGE_CODE+'.tsv'\n",
        "TEXT_COLUMN = 'text'\n",
        "\n",
        "!python starter_kit/run_predict.py \\\n",
        "  --model_path {OUTPUT_DIR} \\\n",
        "  --file_name {FILE_NAME} \\\n",
        "  --text_column {TEXT_COLUMN} \\\n",
        "  --lang_code {LANGUAGE_CODE}"
      ],
      "metadata": {
        "id": "LYL3KcmoWXK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40553a07-776f-45e5-9607-72b81fd9f4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/afrisent-semeval-2023\n",
            "***** Running Prediction *****\n",
            "  Num examples = 2677\n",
            "  Batch size = 8\n",
            "100% 335/335 [00:05<00:00, 58.50it/s]\n",
            "creating submission directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Congratulations. You now trained sentiment classifier and predict on the unlabelled tweets.\n",
        "\n",
        "- The prediction file (pred_lang.tsv) is in \"afrisenti-semval-2023\" folder under \"submissions\" folder. For example, since we trained hausa (ha), you will be able to see pred_ha.tsv file ready for submission. The submission file is in the format below:\n",
        "\n",
        "<center>\n",
        "\n",
        "|ID | label |\n",
        "|--- | --- |\n",
        "|hau_dev_00001| negative |\n",
        "|hau_dev_00002| positive |\n",
        "|... | ... |\n",
        "\n",
        "</center>\n",
        "\n",
        "- Inside the same folder, you will also see a file \"ha_predictions.tsv\" with the format below to see tweets with corresponding sentiment predictions. This file is not for submission.\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "|ID | text | label |\n",
        "|--- | --- | --- | \n",
        "|hau_dev_00001| @user Allah Miki albarkah üôèüôèüôè |  positive |\n",
        "|hau_dev_00002| @user Kidan ma zai dadiüòÇ\t |  negative |\n",
        "|... | ... | ... |\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JpoGeFwSEfVt"
      }
    }
  ]
}